{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Transforms v2: End-to-end object detection/segmentation example\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Try on [collab](https://colab.research.google.com/github/pytorch/vision/blob/gh-pages/main/_generated_ipynb_notebooks/plot_transforms_e2e.ipynb)\n",
        "    or `go to the end <sphx_glr_download_auto_examples_transforms_plot_transforms_e2e.py>` to download the full example code.</p></div>\n",
        "\n",
        "Object detection and segmentation tasks are natively supported:\n",
        "``torchvision.transforms.v2`` enables jointly transforming images, videos,\n",
        "bounding boxes, and masks.\n",
        "\n",
        "This example showcases an end-to-end instance segmentation training case using\n",
        "Torchvision utils from ``torchvision.datasets``, ``torchvision.models`` and\n",
        "``torchvision.transforms.v2``. Everything covered here can be applied similarly\n",
        "to object detection or semantic segmentation tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "HEIGHT = 3984\n",
        "WIDTH  = 5312\n",
        "\n",
        "SCALE = 0.75"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install torch torchvision matplotlib tqdm pycocotools transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "\n",
        "from torchvision import models, datasets, tv_tensors\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# This loads fake data for illustration purposes of this example. In practice, you'll have\n",
        "# to replace this with the proper data.\n",
        "# If you're trying to run that on collab, you can download the assets and the\n",
        "# helpers from https://github.com/pytorch/vision/tree/main/gallery/\n",
        "ROOT = pathlib.Path(\"../data\") / \"flight263_COCO\"\n",
        "IMAGES_PATH = str(ROOT / \"img\")\n",
        "RAW_ANNOTATIONS_PATH = str(ROOT / \"annotations\" / \"instances_default.json\")\n",
        "ANNOTATIONS_PATH = ROOT / \"annotations/instances_annotated.json\"\n",
        "from helpers import plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset preparation\n",
        "\n",
        "We start off by loading the :class:`~torchvision.datasets.CocoDetection` dataset to have a look at what it currently\n",
        "returns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(<PIL.Image.Image image mode=RGB size=3984x5312 at 0x7F0DC20ECD00>, [{'id': 1, 'image_id': 26, 'category_id': 1, 'segmentation': [], 'area': 335.8352000000005, 'bbox': [106.13, 2526.16, 17.24, 19.48], 'iscrowd': 0, 'attributes': {'text': '?', 'bg_color': 'orange', 'txt_color': 'white', 'shape': 'circle', 'occluded': False, 'rotation': 0.0, 'track_id': 0, 'keyframe': True}}])\n",
            "type(img) = <class 'PIL.Image.Image'>\n",
            "type(target) = <class 'list'>\n",
            "type(target[0]) = <class 'dict'>\n",
            "target[0].keys() = dict_keys(['id', 'image_id', 'category_id', 'segmentation', 'area', 'bbox', 'iscrowd', 'attributes'])\n"
          ]
        }
      ],
      "source": [
        "coco_dataset = datasets.CocoDetection(IMAGES_PATH, RAW_ANNOTATIONS_PATH)\n",
        "\n",
        "sample = coco_dataset[25] # Not all images have annotations\n",
        "print(sample)\n",
        "img, target = sample\n",
        "print(f\"{type(img) = }\\n{type(target) = }\\n{type(target[0]) = }\\n{target[0].keys() = }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a subset with only images that have bbox annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_annotated_subset():    \n",
        "    idx_has_ann = []\n",
        "    for i, entry in tqdm(enumerate(coco_dataset)):\n",
        "        if len(entry[1]) > 0:\n",
        "            idx_has_ann += [i]\n",
        "\n",
        "\n",
        "    print(len(idx_has_ann))\n",
        "\n",
        "    import json\n",
        "\n",
        "    with open(RAW_ANNOTATIONS_PATH, \"r\") as f:\n",
        "        instances = json.load(f)\n",
        "\n",
        "    idxs = [x+1 for x in idx_has_ann]\n",
        "    instances[\"images\"] = [x for x in instances[\"images\"] if x[\"id\"] in idxs]\n",
        "\n",
        "    with open(ROOT / \"annotations/instances_annotated.json\", \"w\") as f:\n",
        "        json.dump(instances, f)\n",
        "\n",
        "if not os.path.isfile(ROOT / \"annotations/instances_annotated.json\"):\n",
        "    create_annotated_subset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "(<PIL.Image.Image image mode=RGB size=3984x5312 at 0x7F0EC4AB3A60>, [{'id': 1, 'image_id': 26, 'category_id': 1, 'segmentation': [], 'area': 335.8352000000005, 'bbox': [106.13, 2526.16, 17.24, 19.48], 'iscrowd': 0, 'attributes': {'text': '?', 'bg_color': 'orange', 'txt_color': 'white', 'shape': 'circle', 'occluded': False, 'rotation': 0.0, 'track_id': 0, 'keyframe': True}}])\n",
            "type(img) = <class 'PIL.Image.Image'>\n",
            "type(target) = <class 'list'>\n",
            "type(target[0]) = <class 'dict'>\n",
            "target[0].keys() = dict_keys(['id', 'image_id', 'category_id', 'segmentation', 'area', 'bbox', 'iscrowd', 'attributes'])\n"
          ]
        }
      ],
      "source": [
        "coco_dataset = datasets.CocoDetection(IMAGES_PATH, ROOT / \"annotations/instances_annotated.json\")\n",
        "\n",
        "# TODO: create sliding window as `transforms` arg\n",
        "\n",
        "sample = coco_dataset[0] # Not all images have annotations\n",
        "print(sample)\n",
        "img, target = sample\n",
        "print(f\"{type(img) = }\\n{type(target) = }\\n{type(target[0]) = }\\n{target[0].keys() = }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Torchvision datasets preserve the data structure and types as it was intended\n",
        "by the datasets authors. So by default, the output structure may not always be\n",
        "compatible with the models or the transforms.\n",
        "\n",
        "To overcome that, we can use the\n",
        ":func:`~torchvision.datasets.wrap_dataset_for_transforms_v2` function. For\n",
        ":class:`~torchvision.datasets.CocoDetection`, this changes the target\n",
        "structure to a single dictionary of lists:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type(img) = <class 'PIL.Image.Image'>\n",
            "type(target) = <class 'dict'>\n",
            "target.keys() = dict_keys(['boxes', 'labels'])\n",
            "type(target['boxes']) = <class 'torchvision.tv_tensors._bounding_boxes.BoundingBoxes'>\n",
            "type(target['labels']) = <class 'torch.Tensor'>\n"
          ]
        }
      ],
      "source": [
        "dataset = datasets.wrap_dataset_for_transforms_v2(coco_dataset, target_keys=(\"boxes\", \"labels\"))\n",
        "\n",
        "sample = dataset[0]\n",
        "img, target = sample\n",
        "print(f\"{type(img) = }\\n{type(target) = }\\n{target.keys() = }\")\n",
        "print(f\"{type(target['boxes']) = }\\n{type(target['labels']) = }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We used the ``target_keys`` parameter to specify the kind of output we're\n",
        "interested in. Our dataset now returns a target which is dict where the values\n",
        "are `TVTensors <what_are_tv_tensors>` (all are :class:`torch.Tensor`\n",
        "subclasses). We're dropped all unncessary keys from the previous output, but\n",
        "if you need any of the original keys e.g. \"image_id\", you can still ask for\n",
        "it.\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>If you just want to do detection, you don't need and shouldn't pass\n",
        "    \"masks\" in ``target_keys``: if masks are present in the sample, they will\n",
        "    be transformed, slowing down your transformations unnecessarily.</p></div>\n",
        "\n",
        "As baseline, let's have a look at a sample without transformations:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import helpers\n",
        "# helpers.plot([dataset[0], dataset[10]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transforms\n",
        "\n",
        "Let's now define our pre-processing transforms. All the transforms know how\n",
        "to handle images, bouding boxes and masks when relevant.\n",
        "\n",
        "Transforms are typically passed as the ``transforms`` parameter of the\n",
        "dataset so that they can leverage multi-processing from the\n",
        ":class:`torch.utils.data.DataLoader`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "transforms = v2.Compose(\n",
        "    [\n",
        "        v2.ToImage(),\n",
        "        v2.Resize(int(HEIGHT * SCALE)),\n",
        "        # v2.RandomPhotometricDistort(p=1),\n",
        "        v2.RandomPerspective(distortion_scale=0.6, p=1.0),\n",
        "        v2.RandomRotation(degrees=(0, 180)),\n",
        "        v2.RandomZoomOut(fill={tv_tensors.Image: (123, 117, 104), \"others\": 0}),\n",
        "        v2.RandomIoUCrop(),\n",
        "        v2.RandomHorizontalFlip(p=1),\n",
        "        v2.SanitizeBoundingBoxes(),\n",
        "        v2.ToDtype(torch.float32, scale=True),\n",
        "    ]\n",
        ")\n",
        "\n",
        "dataset = datasets.CocoDetection(IMAGES_PATH, ANNOTATIONS_PATH, transforms=transforms)\n",
        "dataset = datasets.wrap_dataset_for_transforms_v2(dataset, target_keys=[\"boxes\", \"labels\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A few things are worth noting here:\n",
        "\n",
        "- We're converting the PIL image into a\n",
        "  :class:`~torchvision.transforms.v2.Image` object. This isn't strictly\n",
        "  necessary, but relying on Tensors (here: a Tensor subclass) will\n",
        "  `generally be faster <transforms_perf>`.\n",
        "- We are calling :class:`~torchvision.transforms.v2.SanitizeBoundingBoxes` to\n",
        "  make sure we remove degenerate bounding boxes, as well as their\n",
        "  corresponding labels and masks.\n",
        "  :class:`~torchvision.transforms.v2.SanitizeBoundingBoxes` should be placed\n",
        "  at least once at the end of a detection pipeline; it is particularly\n",
        "  critical if :class:`~torchvision.transforms.v2.RandomIoUCrop` was used.\n",
        "\n",
        "Let's look how the sample looks like with our augmentation pipeline in place:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# helpers.plot([dataset[25], dataset[28]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the color of the images were distorted, zoomed in or out, and flipped.\n",
        "The bounding boxes and the masks were transformed accordingly. And without any further ado, we can start training.\n",
        "\n",
        "## Data loading and training loop\n",
        "\n",
        "Below we're using Mask-RCNN which is an instance segmentation model, but\n",
        "everything we've covered in this tutorial also applies to object detection and\n",
        "semantic segmentation tasks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    del train_one_epoch\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import torch.distributed as dist\n",
        "from engine import train_one_epoch, evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset, test_dataset = tuple(torch.utils.data.random_split(dataset, [0.8,0.2]))\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=2,\n",
        "    drop_last=True, # Drop remainder\n",
        "    # We need a custom collation function here, since the object detection\n",
        "    # models expect a sequence of images and target dictionaries. The default\n",
        "    # collation function tries to torch.stack() the individual elements,\n",
        "    # which fails in general for object detection, because the number of bouding\n",
        "    # boxes varies between the images of a same batch.\n",
        "    collate_fn=lambda batch: tuple(zip(*batch)),\n",
        ")\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=2,\n",
        "    drop_last=True, # Drop remainder\n",
        "    # We need a custom collation function here, since the object detection\n",
        "    # models expect a sequence of images and target dictionaries. The default\n",
        "    # collation function tries to torch.stack() the individual elements,\n",
        "    # which fails in general for object detection, because the number of bouding\n",
        "    # boxes varies between the images of a same batch.\n",
        "    collate_fn=lambda batch: tuple(zip(*batch)),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train on the GPU or on the CPU, if a GPU is not available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch, gc\n",
        "if model:\n",
        "    del model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = models.get_model(\"ssdlite320_mobilenet_v3_large\", weights=None, weights_backbone=None)\n",
        "\n",
        "# !pip install -U ultralytics\n",
        "# model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SSD(\n",
              "  (backbone): SSDLiteFeatureExtractorMobileNet(\n",
              "    (features): Sequential(\n",
              "      (0): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
              "              (1): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
              "              (1): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (2): Conv2dNormActivation(\n",
              "              (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (3): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(72, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
              "              (1): BatchNorm2d(72, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (2): Conv2dNormActivation(\n",
              "              (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (4): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(72, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
              "              (1): BatchNorm2d(72, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): ReLU()\n",
              "              (scale_activation): Hardsigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(40, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (5): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(120, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
              "              (1): BatchNorm2d(120, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): ReLU()\n",
              "              (scale_activation): Hardsigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(40, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (6): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(120, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
              "              (1): BatchNorm2d(120, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): ReLU()\n",
              "              (scale_activation): Hardsigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(40, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (7): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(240, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
              "              (1): BatchNorm2d(240, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (2): Conv2dNormActivation(\n",
              "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (8): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(200, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
              "              (1): BatchNorm2d(200, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (2): Conv2dNormActivation(\n",
              "              (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (9): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(184, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
              "              (1): BatchNorm2d(184, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (2): Conv2dNormActivation(\n",
              "              (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (10): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(184, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
              "              (1): BatchNorm2d(184, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (2): Conv2dNormActivation(\n",
              "              (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (11): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): ReLU()\n",
              "              (scale_activation): Hardsigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(112, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (12): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
              "              (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): ReLU()\n",
              "              (scale_activation): Hardsigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(112, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (13): Conv2dNormActivation(\n",
              "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): Sequential(\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
              "            (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(672, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
              "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): ReLU()\n",
              "              (scale_activation): Hardsigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): Conv2dNormActivation(\n",
              "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (1): Conv2dNormActivation(\n",
              "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
              "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): ReLU()\n",
              "              (scale_activation): Hardsigmoid()\n",
              "            )\n",
              "            (3): Conv2dNormActivation(\n",
              "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (extra): ModuleList(\n",
              "      (0): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(480, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
              "          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
              "          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
              "          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (3): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
              "          (1): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (anchor_generator): DefaultBoxGenerator(aspect_ratios=[[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]], clip=True, scales=[0.2, 0.35, 0.5, 0.65, 0.8, 0.95, 1.0], steps=None)\n",
              "  (head): SSDLiteHead(\n",
              "    (classification_head): SSDLiteClassificationHead(\n",
              "      (module_list): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
              "            (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(672, 546, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "            (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(480, 546, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
              "            (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(512, 546, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (3-4): 2 x Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "            (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(256, 546, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (5): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
              "            (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(128, 546, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (regression_head): SSDLiteRegressionHead(\n",
              "      (module_list): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
              "            (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(672, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "            (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(480, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
              "            (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (3-4): 2 x Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "            (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(256, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (5): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
              "            (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(128, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
              "      Resize(min_size=(320,), max_size=320, mode='bilinear')\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# move model to the right device\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Adam.__init__() got an unexpected keyword argument 'momentum'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m params \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad]\n\u001b[0;32m----> 2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0005\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# and a learning rate scheduler\u001b[39;00m\n\u001b[1;32m     10\u001b[0m lr_scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(\n\u001b[1;32m     11\u001b[0m     optimizer,\n\u001b[1;32m     12\u001b[0m     step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     13\u001b[0m     gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m     14\u001b[0m )\n",
            "\u001b[0;31mTypeError\u001b[0m: Adam.__init__() got an unexpected keyword argument 'momentum'"
          ]
        }
      ],
      "source": [
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.Adam(\n",
        "    params,\n",
        "    lr=0.005,\n",
        "    # momentum=0.9,\n",
        "    weight_decay=0.0005\n",
        ")\n",
        "\n",
        "# and a learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=3,\n",
        "    gamma=0.1\n",
        ")\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]=\"expandable_segments:True\"\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_test, device=device)\n",
        "\n",
        "    # print(f\"{[img.shape for img in imgs] = }\")\n",
        "    # print(f\"{[type(target) for target in targets] = }\")\n",
        "    # for name, loss_val in loss_dict.items():\n",
        "    #     print(f\"{name:<20}{loss_val:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
        "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
        "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
        "print(torch.cuda.memory_summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training References\n",
        "\n",
        "From there, you can check out the [torchvision references](https://github.com/pytorch/vision/tree/main/references) where you'll find\n",
        "the actual training scripts we use to train our models.\n",
        "\n",
        "**Disclaimer** The code in our references is more complex than what you'll\n",
        "need for your own use-cases: this is because we're supporting different\n",
        "backends (PIL, tensors, TVTensors) and different transforms namespaces (v1 and\n",
        "v2). So don't be afraid to simplify and only keep what you need.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
