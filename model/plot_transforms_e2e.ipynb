{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Transforms v2: End-to-end object detection/segmentation example\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Try on [collab](https://colab.research.google.com/github/pytorch/vision/blob/gh-pages/main/_generated_ipynb_notebooks/plot_transforms_e2e.ipynb)\n",
        "    or `go to the end <sphx_glr_download_auto_examples_transforms_plot_transforms_e2e.py>` to download the full example code.</p></div>\n",
        "\n",
        "Object detection and segmentation tasks are natively supported:\n",
        "``torchvision.transforms.v2`` enables jointly transforming images, videos,\n",
        "bounding boxes, and masks.\n",
        "\n",
        "This example showcases an end-to-end instance segmentation training case using\n",
        "Torchvision utils from ``torchvision.datasets``, ``torchvision.models`` and\n",
        "``torchvision.transforms.v2``. Everything covered here can be applied similarly\n",
        "to object detection or semantic segmentation tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install torch torchvision matplotlib tqdm pycocotools transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "HEIGHT = 3984\n",
        "WIDTH  = 5312\n",
        "\n",
        "SCALE = 0.75"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "\n",
        "from torchvision import models, datasets, tv_tensors\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# This loads fake data for illustration purposes of this example. In practice, you'll have\n",
        "# to replace this with the proper data.\n",
        "# If you're trying to run that on collab, you can download the assets and the\n",
        "# helpers from https://github.com/pytorch/vision/tree/main/gallery/\n",
        "ROOT = pathlib.Path(\"../data\") / \"flight263_COCO\"\n",
        "IMAGES_PATH = str(ROOT / \"img\")\n",
        "RAW_ANNOTATIONS_PATH = str(ROOT / \"annotations\" / \"instances_default.json\")\n",
        "ANNOTATIONS_PATH = ROOT / \"annotations/instances_annotated.json\"\n",
        "from helpers import plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset preparation\n",
        "\n",
        "We start off by loading the :class:`~torchvision.datasets.CocoDetection` dataset to have a look at what it currently\n",
        "returns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "coco_dataset = datasets.CocoDetection(IMAGES_PATH, RAW_ANNOTATIONS_PATH)\n",
        "\n",
        "sample = coco_dataset[25] # Not all images have annotations\n",
        "print(sample)\n",
        "img, target = sample\n",
        "print(f\"{type(img) = }\\n{type(target) = }\\n{type(target[0]) = }\\n{target[0].keys() = }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a subset with only images that have bbox annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_annotated_subset():    \n",
        "    idx_has_ann = []\n",
        "    for i, entry in tqdm(enumerate(coco_dataset)):\n",
        "        if len(entry[1]) > 0:\n",
        "            idx_has_ann += [i]\n",
        "\n",
        "\n",
        "    print(len(idx_has_ann))\n",
        "\n",
        "    import json\n",
        "\n",
        "    with open(RAW_ANNOTATIONS_PATH, \"r\") as f:\n",
        "        instances = json.load(f)\n",
        "\n",
        "    idxs = [x+1 for x in idx_has_ann]\n",
        "    instances[\"images\"] = [x for x in instances[\"images\"] if x[\"id\"] in idxs]\n",
        "\n",
        "    with open(ROOT / \"annotations/instances_annotated.json\", \"w\") as f:\n",
        "        json.dump(instances, f)\n",
        "\n",
        "if not os.path.isfile(ROOT / \"annotations/instances_annotated.json\"):\n",
        "    create_annotated_subset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "coco_dataset = datasets.CocoDetection(IMAGES_PATH, ROOT / \"annotations/instances_annotated.json\")\n",
        "\n",
        "# TODO: create sliding window as `transforms` arg\n",
        "\n",
        "sample = coco_dataset[0] # Not all images have annotations\n",
        "print(sample)\n",
        "img, target = sample\n",
        "print(f\"{type(img) = }\\n{type(target) = }\\n{type(target[0]) = }\\n{target[0].keys() = }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Torchvision datasets preserve the data structure and types as it was intended\n",
        "by the datasets authors. So by default, the output structure may not always be\n",
        "compatible with the models or the transforms.\n",
        "\n",
        "To overcome that, we can use the\n",
        ":func:`~torchvision.datasets.wrap_dataset_for_transforms_v2` function. For\n",
        ":class:`~torchvision.datasets.CocoDetection`, this changes the target\n",
        "structure to a single dictionary of lists:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = datasets.wrap_dataset_for_transforms_v2(coco_dataset, target_keys=(\"boxes\", \"labels\", \"image_id\"))\n",
        "\n",
        "sample = dataset[0]\n",
        "img, target = sample\n",
        "print(f\"{type(img) = }\\n{type(target) = }\\n{target.keys() = }\")\n",
        "print(f\"{type(target['boxes']) = }\\n{type(target['labels']) = }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We used the ``target_keys`` parameter to specify the kind of output we're\n",
        "interested in. Our dataset now returns a target which is dict where the values\n",
        "are `TVTensors <what_are_tv_tensors>` (all are :class:`torch.Tensor`\n",
        "subclasses). We're dropped all unncessary keys from the previous output, but\n",
        "if you need any of the original keys e.g. \"image_id\", you can still ask for\n",
        "it.\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>If you just want to do detection, you don't need and shouldn't pass\n",
        "    \"masks\" in ``target_keys``: if masks are present in the sample, they will\n",
        "    be transformed, slowing down your transformations unnecessarily.</p></div>\n",
        "\n",
        "As baseline, let's have a look at a sample without transformations:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import helpers\n",
        "# helpers.plot([dataset[0], dataset[10]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transforms\n",
        "\n",
        "Let's now define our pre-processing transforms. All the transforms know how\n",
        "to handle images, bouding boxes and masks when relevant.\n",
        "\n",
        "Transforms are typically passed as the ``transforms`` parameter of the\n",
        "dataset so that they can leverage multi-processing from the\n",
        ":class:`torch.utils.data.DataLoader`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "transforms = v2.Compose(\n",
        "    [\n",
        "        v2.ToImage(),\n",
        "        v2.Resize(int(HEIGHT * SCALE)),\n",
        "        # v2.RandomPhotometricDistort(p=1),\n",
        "        v2.RandomPerspective(distortion_scale=0.6, p=1.0),\n",
        "        v2.RandomRotation(degrees=(0, 180)),\n",
        "        v2.RandomZoomOut(fill={tv_tensors.Image: (123, 117, 104), \"others\": 0}),\n",
        "        v2.RandomIoUCrop(),\n",
        "        v2.RandomHorizontalFlip(p=1),\n",
        "        v2.SanitizeBoundingBoxes(),\n",
        "        v2.ToDtype(torch.float32, scale=True),\n",
        "    ]\n",
        ")\n",
        "\n",
        "dataset = datasets.CocoDetection(IMAGES_PATH, ANNOTATIONS_PATH, transforms=transforms)\n",
        "dataset = datasets.wrap_dataset_for_transforms_v2(dataset, target_keys=(\"boxes\", \"labels\", \"image_id\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A few things are worth noting here:\n",
        "\n",
        "- We're converting the PIL image into a\n",
        "  :class:`~torchvision.transforms.v2.Image` object. This isn't strictly\n",
        "  necessary, but relying on Tensors (here: a Tensor subclass) will\n",
        "  `generally be faster <transforms_perf>`.\n",
        "- We are calling :class:`~torchvision.transforms.v2.SanitizeBoundingBoxes` to\n",
        "  make sure we remove degenerate bounding boxes, as well as their\n",
        "  corresponding labels and masks.\n",
        "  :class:`~torchvision.transforms.v2.SanitizeBoundingBoxes` should be placed\n",
        "  at least once at the end of a detection pipeline; it is particularly\n",
        "  critical if :class:`~torchvision.transforms.v2.RandomIoUCrop` was used.\n",
        "\n",
        "Let's look how the sample looks like with our augmentation pipeline in place:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# helpers.plot([dataset[25], dataset[28]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the color of the images were distorted, zoomed in or out, and flipped.\n",
        "The bounding boxes and the masks were transformed accordingly. And without any further ado, we can start training.\n",
        "\n",
        "## Data loading and training loop\n",
        "\n",
        "Below we're using Mask-RCNN which is an instance segmentation model, but\n",
        "everything we've covered in this tutorial also applies to object detection and\n",
        "semantic segmentation tasks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    del train_one_epoch\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import torch.distributed as dist\n",
        "from engine import train_one_epoch, evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset, test_dataset = tuple(torch.utils.data.random_split(dataset, [0.8,0.2]))\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=2,\n",
        "    drop_last=True, # Drop remainder\n",
        "    # We need a custom collation function here, since the object detection\n",
        "    # models expect a sequence of images and target dictionaries. The default\n",
        "    # collation function tries to torch.stack() the individual elements,\n",
        "    # which fails in general for object detection, because the number of bouding\n",
        "    # boxes varies between the images of a same batch.\n",
        "    collate_fn=lambda batch: tuple(zip(*batch)),\n",
        ")\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=2,\n",
        "    drop_last=True, # Drop remainder\n",
        "    # We need a custom collation function here, since the object detection\n",
        "    # models expect a sequence of images and target dictionaries. The default\n",
        "    # collation function tries to torch.stack() the individual elements,\n",
        "    # which fails in general for object detection, because the number of bouding\n",
        "    # boxes varies between the images of a same batch.\n",
        "    collate_fn=lambda batch: tuple(zip(*batch)),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train on the GPU or on the CPU, if a GPU is not available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch, gc\n",
        "if model:\n",
        "    del model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = models.get_model(\"ssdlite320_mobilenet_v3_large\", weights=None, weights_backbone=None)\n",
        "\n",
        "# !pip install -U ultralytics\n",
        "# model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# move model to the right device\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.Adam(\n",
        "    params,\n",
        "    lr=0.005,\n",
        "    # momentum=0.9,\n",
        "    weight_decay=0.0005\n",
        ")\n",
        "\n",
        "# and a learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=3,\n",
        "    gamma=0.1\n",
        ")\n",
        "\n",
        "num_epochs = 1\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]=\"expandable_segments:True\"\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_test, device=device)\n",
        "\n",
        "    # print(f\"{[img.shape for img in imgs] = }\")\n",
        "    # print(f\"{[type(target) for target in targets] = }\")\n",
        "    # for name, loss_val in loss_dict.items():\n",
        "    #     print(f\"{name:<20}{loss_val:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.transforms.v2 import functional as F\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "import matplotlib.pyplot as plt\n",
        "FONT = \"/usr/share/fonts/truetype/ubuntu/UbuntuMono-R.ttf\"\n",
        "\n",
        "def compare_pred(entry):\n",
        "    if isinstance(entry, tuple):\n",
        "        images, targets = entry\n",
        "        img = images[0]\n",
        "        # targets = targets[0]\n",
        "        img = F.to_image(img)\n",
        "        if img.dtype.is_floating_point and img.min() < 0:\n",
        "            # Poor man's re-normalization for the colors to be OK-ish. This\n",
        "            # is useful for images coming out of Normalize()\n",
        "            img -= img.min()\n",
        "            img /= img.max()\n",
        "\n",
        "        img = F.to_dtype(img, torch.uint8, scale=True)\n",
        "\n",
        "        for target in targets:\n",
        "            if isinstance(target, dict):\n",
        "                boxes = target.get(\"boxes\")\n",
        "            elif isinstance(target, tv_tensors.BoundingBoxes):\n",
        "                boxes = target\n",
        "            else:\n",
        "                raise ValueError(f\"Unexpected target type: {type(target)}\")\n",
        "        \n",
        "            if boxes is not None:\n",
        "                img = draw_bounding_boxes(img, boxes, [str(x) for x in range(len(boxes))], \n",
        "                                          colors=\"green\", width=3, font=FONT, font_size=100)\n",
        "\n",
        "        # model.eval()\n",
        "        # images = iter(map(eval_transforms, images))\n",
        "        # images = list(img.to(device) for img in images)\n",
        "\n",
        "        # if torch.cuda.is_available():\n",
        "        #     torch.cuda.synchronize()\n",
        "        # predictions = model(images)\n",
        "        # for pred in predictions[:2]:\n",
        "        #     pred_boxes = pred[\"boxes\"].long()\n",
        "        #     img = draw_bounding_boxes(img, pred_boxes, colors=\"red\")\n",
        "        \n",
        "    plt.figure(figsize=(12, 12))\n",
        "    plt.imshow(img.permute(1, 2, 0))\n",
        "\n",
        "compare_pred(next(iter(data_loader_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from torchvision.io import read_image\n",
        "\n",
        "image = read_image(\"../data/flight263_COCO/img/flight_263_im00668.jpg\")\n",
        "\n",
        "eval_transforms = v2.Compose(\n",
        "    [\n",
        "        v2.ToImage(),\n",
        "        v2.Resize(int(HEIGHT * SCALE)),\n",
        "        # v2.SanitizeBoundingBoxes(),\n",
        "        v2.ToDtype(torch.float32, scale=True),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    x = eval_transforms(image)\n",
        "    # convert RGBA -> RGB and move to device\n",
        "    x = x[:3, ...].to(device)\n",
        "    predictions = model([x, ])\n",
        "    pred = predictions[0]\n",
        "\n",
        "image = v2.Resize(int(HEIGHT * SCALE))(image)\n",
        "\n",
        "image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\n",
        "pred_labels = [f\"target: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\n",
        "pred_boxes = pred[\"boxes\"].long()\n",
        "output_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.imshow(output_image.permute(1, 2, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
        "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
        "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
        "print(torch.cuda.memory_summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training References\n",
        "\n",
        "From there, you can check out the [torchvision references](https://github.com/pytorch/vision/tree/main/references) where you'll find\n",
        "the actual training scripts we use to train our models.\n",
        "\n",
        "**Disclaimer** The code in our references is more complex than what you'll\n",
        "need for your own use-cases: this is because we're supporting different\n",
        "backends (PIL, tensors, TVTensors) and different transforms namespaces (v1 and\n",
        "v2). So don't be afraid to simplify and only keep what you need.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
